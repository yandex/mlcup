# Введение

Один из ярких современных трендов в машинном обучении — использование больших объемов данных, собранных в автоматическом режиме без ручной разметки. Например, модели, основанные на трансформерах, совершили переворот в сфере NLP за счет предобучения на огромных корпусах текстовых данных. Предобученные модели, такие как BERT и GPT, повсеместно используются для эффективного дообучения на самых разных целевых задачах. Идею обучения таких «универсальных» моделей активно продолжают развивать и в других доменах, в том числе в домене изображений, видео, и для мультимодальных данных.

В этой задаче предлагаем вам обучить такую универсальную модель, которая способна решать задачу классификации изображений в режиме zero-shot. Например, по одному только названию класса (без дообучения) модель должна уметь отличать изображения варежек от изображений перчаток, фотографии Москвы от фотографий Казани, фотографии Яндекс.Станции Макс от фотографий Яндекс.Станции Мини. Модель будет оцениваться по точности классификации на нескольких, не известных заранее наборах классов, доступ к названиям которых будет дан лишь во время автоматической оценки на приватном наборе данных. Участники получат публичную часть тестовых данных, на которой они смогут локально протестировать и провалидировать свое решение. Однако финально решения будут оцениваться путем запуска кода обученной модели на приватной части тестовых данных. Участники должны будут загрузить код своего решения в тестирующую систему, где он будет запущен в стандартном изолированном окружении.

На каких же данных обучать такую модель? К сожалению, пока объем открытых данных в этой области оставляет желать лучшего (например, датасеты MS-COCO и Visual Genome содержат всего порядка 100 тысяч размеченных пар «изображение + текст»), а наборы пар «изображение + заголовок», которые можно собрать скрапингом интернета, часто получаются шумными и малоинформативными. Поэтому в рамках CV-трека ML-чемпионата мы выложим в открытый доступ многомиллионный набор мультимодальных данных (релевантных пар «текстовый запрос + изображение»), собранных по данным логов Яндекс.Картинок. Пользователи, как правило, кликают на самые релевантные изображения из поисковой выдачи — это обеспечивает дополнительную фильтрацию нерелевантных запросу изображений и уменьшает шум в данных. Ваша задача — извлечь из этих данных максимально полезный сигнал и обучить модель, которая продемонстрирует максимальную обобщающую способность. 

# Задача

## Обучающие данные

В этой задаче мы предлагаем вам обучить мультимодальную модель на парах `[текст поискового запроса, релевантное запросу изображение]`, полученных сопоставлением наиболее релевантных изображений и пользовательских запросов по реальным кликовым данным. Отметим, что предлагаемый датасет сильно отличается от "Image Captioning" датасетов (таких как MS COCO) спецификой сбора данных: тексты представляют из себя не подписи к изображениям, а реальные поисковые запросы пользователей, что определяет специфику данных.

В датасете представлено более 5 миллионов уникальных изображений и более 20 миллионов уникальных пар `[текст, изображение]`. 

## Оценка качества

Обученная вами модель будет тестироваться на задаче zero-shot классификации изображений, то есть вам необходимо обучить модель, способную классифицировать изображения, имея в распоряжении лишь список с русскоязычными названиями классов. В публичном зачете модель будет замеряться на двух наборах тестовых данных:

1. Датасет [Сaltech101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) (подвыборка из 518 изображений)
2. Датасет RussianSingers (датасет из 625 изображений российских певцов)

В приватном зачете (который является финальным) качество модели будет замеряться на нескольких закрытых наборах тестовых данных. Приватные тестовые данные будут доступны только во время оценки кода решения в изолированном окружении, названия классов в приватных наборах данных опубликованы **не будут**. Во время проведения соревнования лидерборд будет строиться по оценке качества на публичном наборе данных, после завершения чемпионата все решения будут автоматически перетестированы на приватном наборе данных. 


# Train данные

Исходные данные для обучения представлены в виде файла `metadata.json` (в формате json-lines) следующей структуры:

```json
{"image": 1, "queries": ["запрос1", "запрос2", "запрос3"]}
{"image": 2, "queries": ["запрос1", "запрос2", "запрос3"]}
...
```

В отдельном файле `images.json` даны ссылки на исходные изображения:

```json
{"image": 1, "url": "http://path.to/image1.jpg"}
{"image": 2, "url": "http://path.to/image2.jpg"}
...
```

Доступность всех изображений по предоставленным ссылкам не гарантируется. **Все права на изображения принадлежат их правообладателям. Распространение, коммерческое и личное использование данных вне соревнования недопустимо.**

# Eval данные

Данные, на которых замеряется качество zero-shot классификации, представляют из себя несколько датасетов с изображениями. Каждый датасет расположен в отдельной директории с содержимым вида:

```
.
├── classes.json
└── img
    ├── 01204c5c-bdcd-4535-b981-318d12d16b40.jpg
    ├── 0135b8ce-1f9e-485a-81a1-82c302d44128.jpg
    ├── 0168b6e5-530b-4fde-801a-56f5c2d0762c.jpg
    ├── 01975fb0-ab0c-4b67-b159-e704d52b7660.jpg
...
```

В файле `classes.json` дан список русскоязычных названий классов (нумерация с 0). Для каждого из файлов директории `img` необходимо предсказать один (и только один) из классов. В качестве итоговой оценки используется средняя точность (precision) по всему набору датасетов, умноженная на 100. Код замера оценки можно найти в [репозитории](https://github.com/yandex/mlcup/blob/main/cv/contest/evaluate_predictions.py) с бейзлайн-решением.

## Разбиение на public и private наборы

Все датасеты, на которых производится оценка качества, разбиты на два набора, "public" (Сaltech101 и RussianSingers), и "private", как описано выше. Набор датасетов и классов в них в приватном наборе опубликован не будет. Важно учитывать, что набор датасетов в приватной части отличается от набора датасетов в публичной части. В частности, решения, основанные на том, что набор классов в публичной части известен заранее, а также решения, основанные на предобученных англоязычных моделях, вероятно, покажут очень низкое качество на приватном наборе данных.

Маленькая подвыборка public датасета, на которой можно тестировать работоспособность решения, доступна в [репозитории](https://github.com/yandex/mlcup/tree/main/cv/contest/data/public_subset). Полный датасет доступен модели во время запуска кода в окружении Яндекс.Контеста.


## Формат отправки решения

Для отправки в тестирующую систему необходимо подготовить архив, в **корневой** директории которого присутсвуют файлы `setup.sh` и `predict.sh`. Первый файл `setup.sh` будет вызван без аргументов и может быть использован для настройки окружения, установки дополнительных пакетов, которые необходимо добавить в архив. Второй файл `predict.sh` будет вызван с двумя аргументами: первый — путь к директории с датасетами, второй — путь к json-файлу, в который необходимо вывести предсказания модели на всех датасетах. Пример формата выходного файла можно найти [здесь](https://github.com/yandex/mlcup/blob/main/cv/contest/predicts.json). Библиотеки, доступные по-умолчанию, описаны в [docker-файле](https://github.com/yandex/mlcup/tree/main/cv/contest/Dockerfile).
Полный пример решения, готового к отправке, можно найти в [репозитории](https://github.com/yandex/mlcup/tree/main/cv/).

# Baseline-решение

Baseline-решение представляет из себя классическую двухбашенную мультимодальную модель, обученную отличать релевантные пары текст+изображение от нерелевантных (contrastive target). В качестве энкодера изображений используется resnet50 (претренированный на imagenet), в качестве энкодера текстов используется модель Bag-of-Words. Код обучения, предобученные веса, скрипты для запуска предсказания классов на наборе данных и оценки качества, а также более подробное техническое описание доступны в [репозитории](https://github.com/yandex/mlcup/tree/main/cv) с baseline-решением.
