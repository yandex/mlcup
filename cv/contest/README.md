# Введение

Один из ярких современных трендов в машинном обучении — использование больших объемов данных, собранных в автоматическом режиме без ручной разметки. Например, модели, основанные на трансформерах, совершили переворот в сфере NLP за счет предобучения на огромных корпусах текстовых данных. Предобученные модели, такие как BERT и GPT, повсеместно используются для эффективного дообучения на самых разных целевых задачах. Идею обучения таких «универсальных» моделей активно продолжают развивать и в других доменах, в том числе в домене изображений, видео, и для мультимодальных данных.

В этой задаче предлагаем вам обучить такую универсальную модель, которая способна решать задачу классификации изображений в режиме zero-shot. Например, по одному только названию класса (без дообучения) модель должна уметь отличать изображения варежек от изображений перчаток, фотографии Москвы от фотографий Казани, фотографии Яндекс.Станции Макс от фотографий Яндекс.Станции Мини. Модель будет оцениваться по точности классификации на нескольких, не известных заранее наборах классов, доступ к названиям которых будет дан лишь во время автоматической оценки на приватном наборе данных. Участники получат публичную часть тестовых данных, на которой они смогут локально протестировать и провалидировать свое решение. Однако финально решения будут оцениваться путем запуска кода обученной модели на приватной части тестовых данных. Участники должны будут загрузить код своего решения в тестирующую систему, где он будет запущен в стандартном изолированном окружении.

На каких же данных обучать такую модель? К сожалению, пока объем открытых данных в этой области оставляет желать лучшего (например, датасеты MS-COCO и Visual Genome содержат всего порядка 100 тысяч размеченных пар «изображение + текст»), а наборы пар «изображение + заголовок», которые можно собрать скрапингом интернета, часто получаются шумными и малоинформативными. Поэтому в рамках CV-трека ML-чемпионата мы выложим в открытый доступ многомиллионный набор мультимодальных данных (релевантных пар «текстовый запрос + изображение»), собранных по данным логов Яндекс.Картинок. Пользователи, как правило, кликают на самые релевантные изображения из поисковой выдачи — это обеспечивает дополнительную фильтрацию нерелевантных запросу изображений и уменьшает шум в данных. Ваша задача — извлечь из этих данных максимально полезный сигнал и обучить модель, которая продемонстрирует максимальную обобщающую способность. 

# Задача

## Обучающие данные

В этой задаче мы предлагаем вам обучить мультимодальную модель на парах `[текст поискового запроса, релевантное запросу изображение]`, полученных сопоставлением наиболее релевантных изображений и пользовательских запросов по реальным кликовым данным. Отметим, что предлагаемый датасет сильно отличается от "Image Captioning" датасетов (таких как MS COCO) спецификой сбора данных: тексты представляют из себя не подписи к изображениям, а реальные поисковые запросы пользователей, что определяет специфику данных.

В датасете представлено более 5 миллионов уникальных изображений и более 20 миллионов уникальных пар `[текст, изображение]`. 

## Оценка качества

Обученная вами модель будет тестироваться на задаче zero-shot классификации изображений, то есть вам необходимо обучить модель, способную классифицировать изображения, имея в распоряжении лишь список с русскоязычными названиями классов.

Все датасеты, на которых производится оценка качества, разбиты на два набора, "публичный", и "приватный". В публичном зачете модель будет замеряться на двух наборах тестовых данных:

1. Датасет [Сaltech101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) (подвыборка из 518 изображений)
2. Датасет RussianSingers (датасет из 625 изображений российских певцов)

Вам предоставляется полный доступ к публичным тестовым данным, поэтому публичный зачет является **иллюстративным** и может быть использован только для предварительной оценки качества и решения и работоспособности модели в системе Яндекс.Контест. Окончательная оценка будет определяться качеством модели на приватном наборе данных. Приватные тестовые данные будут доступны только во время запуска кода участников в изолированном окружении, названия классов в приватных наборах данных опубликованы **не будут**. Во время проведения соревнования лидерборд будет строиться по оценкам на публичном наборе данных, после завершения чемпионата все решения будут автоматически перетестированы на приватном наборе данных. В качестве решения, которое будет использовано в качестве окончательного и будет оценено на приватном наборе данных, будет использоваться **последняя успешная посылка** (со статусом "ОК"). **Будьте внимательны!** Если ваша лучшая посылка не будет последней, она не будет использована для финальной оценки!

Обратите внимание, что высокая оценка решения в публичном зачете **не означает**, что результат в приватном зачете будет также высоким. Высокий (или даже) идеальный результат в публичном зачете может быть достигнут 
* использованием в модели gt-меток
* использованием в модели знания о наборе классов
* обучением модели на дополнительных данных, подбор которых основан на специфике датасетов
* использованием предобученных англоязычных моделей
* др.

Все эти методы с большой вероятностью приведут к ситуации, в которой баллы на приватном наборе данных будут гораздо ниже, чем баллы на публичном наборе. Поэтому их использование настоятельно **не рекомендуется**. 
Для оценки обобщающей способности модели **рекомендуется** проверять качество zero-shot классификации на разнообразных открытых датасетах, таких как ImageNet, MS COCO и др.

Скачать публичные данные можно по ссылке:
[public.tar.gz](https://cvlab.s3.yandex.net/mlcup2021/public.tar.gz)


# Train данные

Исходные данные для обучения представлены в виде файла `metadata.json` (в формате json-lines) следующей структуры:

```json
{"image": 1, "queries": ["запрос1", "запрос2", "запрос3"]}
{"image": 2, "queries": ["запрос1", "запрос2", "запрос3"]}
...
```

В отдельном файле `images.json` даны ссылки на исходные изображения:

```json
{"image": 1, "url": "http://path.to/image1.jpg"}
{"image": 2, "url": "http://path.to/image2.jpg"}
...
```

Доступность всех изображений по предоставленным ссылкам не гарантируется. **Все права на изображения принадлежат их правообладателям. Распространение, коммерческое и личное использование данных вне соревнования недопустимо.**

Скачать данные можно по ссылкам:
[images.json](https://cvlab.s3.yandex.net/mlcup2021/images.json)
[metadata.json](https://cvlab.s3.yandex.net/mlcup2021/metadata.json)


# Eval данные

Данные, на которых замеряется качество zero-shot классификации, представляют из себя несколько датасетов с изображениями. Каждый датасет расположен в **отдельной директории** с содержимым вида:

```
.
├── classes.json
└── img
    ├── 01204c5c-bdcd-4535-b981-318d12d16b40.jpg
    ├── 0135b8ce-1f9e-485a-81a1-82c302d44128.jpg
    ├── 0168b6e5-530b-4fde-801a-56f5c2d0762c.jpg
    ├── 01975fb0-ab0c-4b67-b159-e704d52b7660.jpg
...
```

В файле `classes.json` дан список русскоязычных названий классов (нумерация с 0). Для каждого из файлов директории `img` необходимо предсказать один (и только один) из классов. В качестве итоговой оценки используется средняя точность (precision) по всему набору датасетов, умноженная на 100. Код замера оценки можно найти в [репозитории](https://github.com/yandex/mlcup/blob/main/cv/contest/evaluate_predictions.py) с бейзлайн-решением.


Маленькая подвыборка public датасета, на которой можно тестировать работоспособность решения, доступна в [репозитории](https://github.com/yandex/mlcup/tree/main/cv/contest/data/public_subset). Полный публичный датасет доступен модели во время запуска кода в окружении Яндекс.Контеста, а также его можно скачать по [ссылке](https://cvlab.s3.yandex.net/mlcup2021/public.tar.gz).


## Формат отправки решения

Для отправки в тестирующую систему необходимо подготовить архив, в **корневой** директории которого присутсвуют файлы `setup.sh` и `predict.sh`. Первый файл `setup.sh` будет вызван без аргументов и может быть использован для настройки окружения, установки дополнительных пакетов (которые необходимо добавить в архив). Второй файл `predict.sh` будет вызван с двумя аргументами: первый — путь к директории с датасетами, второй — путь к json-файлу, в который необходимо вывести предсказания модели на всех датасетах. Пример формата выходного файла можно найти [здесь](https://github.com/yandex/mlcup/blob/main/cv/contest/predictsions.json). Библиотеки, доступные по-умолчанию, описаны в [docker-файле](https://github.com/yandex/mlcup/tree/main/cv/contest/Dockerfile).
Полный пример решения, готового к отправке, можно найти в [репозитории](https://github.com/yandex/mlcup/tree/main/cv/).

# Baseline-решение

Baseline-решение представляет из себя классическую двухбашенную мультимодальную модель, обученную отличать релевантные пары "текст+изображение" от нерелевантных (contrastive target). В качестве энкодера изображений используется `resnet50` (претренированный на imagenet), в качестве энкодера текстов используется модель Bag-of-Words. Код обучения, предобученные веса, скрипты для запуска предсказания классов на наборе данных и оценки качества, а также более подробное техническое описание доступны в [репозитории](https://github.com/yandex/mlcup/tree/main/cv) с baseline-решением.


# Ограничения в системе Яндекс.Контест

1. Отправляемые архивы не должны иметь объем, превышающий 700 Мб.
2. Код инференса модели должен уметь использовать несколько CPU-ядер (при наличии), в противном случае имеется риск выхода за максимальное время при тестировании на приватных данных. Код, использующий для инференса распространенные фреймворки, такие как PyTorch и Tensorflow, использует несколько CPU-ядер по умолчанию.
3. Временное ограничение на 1 посылку — 15 минут. Для оценки: в текущем окружении Яндекс.Контеста PyTorch-модель ResNet50 (бейзлайн) работает со скоростью 3 изображения в секунду, что общем количестве изображений 1137 дает время работы 379 секунд (6.5 минут).
4. В изолированном окружении нет доступа к сети. Любые дополнительные пакеты необходимо добавлять в архив с вашей моделью.

