# @package _global_

tokenizer:
  embedding_size: 200
  vocab_size: 200_000

_data:
  tokenizer:
    cls: i2t.data.BPEmbTokenizer
    args:
      model_file: ./tokenizers/ru.wiki.bpe.vs200000.model
      dim: ${tokenizer.embedding_size}
      vs: ${tokenizer.vocab_size}

model:
  text:
    args:
      encoder:
        args:
          pretrained_bpemb_embeddings: True
          embedding_size: ${tokenizer.embedding_size}
          vocab_size: ${tokenizer.vocab_size}
          freeze_embeddings: False
