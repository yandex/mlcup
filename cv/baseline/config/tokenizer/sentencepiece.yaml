# @package _global_

tokenizer:
  embedding_size: 200
  vocab_size: 500_000

_data:
  tokenizer:
    cls: i2t.data.SentencePieceTokenizer
    args:
      model_file: ./tokenizers/sentecepiece_500K.model

model:
  text:
    args:
      encoder:
        args:
          pretrained_bpemb_embeddings: False
          embedding_size: ${tokenizer.embedding_size}
          vocab_size: ${tokenizer.vocab_size}
          freeze_embeddings: False
