{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем скачанный классификатор токсичности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimdi-y/.local/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (2.3.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"trained_roberta/\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"trained_roberta/\").cuda()\n",
    "\n",
    "TOXIC_CLASS=-1\n",
    "TOKENIZATION_TYPE='sentencepiece'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже функции для применения классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import softmax, sigmoid\n",
    "import numpy as np\n",
    "\n",
    "def logits_to_toxic_probas(logits):\n",
    "    if logits.shape[-1] > 1:\n",
    "        activation = lambda x: softmax(x, -1)\n",
    "    else:\n",
    "        activation = sigmoid\n",
    "    return activation(logits)[:, TOXIC_CLASS].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def is_word_start(token):\n",
    "    if TOKENIZATION_TYPE == 'sentencepiece':\n",
    "        return token.startswith('▁')\n",
    "    if TOKENIZATION_TYPE == 'bert':\n",
    "        return not token.startswith('##')\n",
    "    raise ValueError(\"Unknown tokenization type\")\n",
    "\n",
    "\n",
    "def normalize(sentence, max_tokens_per_word=20):\n",
    "    sentence = ''.join(map(lambda c: c if c.isalpha() else ' ', sentence.lower()))\n",
    "    ids = tokenizer(sentence)['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)[1:-1]\n",
    "    \n",
    "    result = []\n",
    "    num_continuation_tokens = 0\n",
    "    for token in tokens:\n",
    "        if not is_word_start(token):\n",
    "            num_continuation_tokens += 1\n",
    "            if num_continuation_tokens < max_tokens_per_word:\n",
    "                result.append(token.lstrip('#▁'))\n",
    "        else:\n",
    "            num_continuation_tokens = 0\n",
    "            result.extend([' ', token.lstrip('▁#')])\n",
    "    \n",
    "    return ''.join(result).strip()\n",
    "\n",
    "def iterate_batches(data, batch_size=40):\n",
    "    batch = []\n",
    "    for x in data:\n",
    "        batch.append(x)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "def predict_toxicity(sentences, batch_size=5, threshold=0.5, return_scores=False, verbose=True, device='cuda'):\n",
    "    results = []\n",
    "    tqdm_fn = tqdm if verbose else lambda x, total: x\n",
    "    for batch in tqdm_fn(iterate_batches(sentences, batch_size), total=np.ceil(len(sentences) / batch_size)):\n",
    "        normlized = [normalize(sent, max_tokens_per_word=5) for sent in batch]\n",
    "        tokenized = tokenizer(normlized, return_tensors='pt', padding=True, max_length=512, truncation=True)\n",
    "        \n",
    "        logits = model.to(device)(**{key: val.to(device) for key, val in tokenized.items()}).logits\n",
    "        preds = logits_to_toxic_probas(logits)\n",
    "        if not return_scores:\n",
    "            preds = preds >= threshold\n",
    "        results.extend(preds)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем тестовый набор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "with open('public_testset.short.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        texts.append(normalize(line)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем токсичность отдельных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c3a7dd4be346aa82d73178e767a8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "words = set()\n",
    "for text in texts:\n",
    "    words.update(text.split())\n",
    "words = sorted(words)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    word_toxicities = predict_toxicity(texts, batch_size=100, return_scores=True)\n",
    "    \n",
    "toxicity = dict(zip(words, word_toxicities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже читаем эмбеддинги слов и описываем функции их обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "stemmer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_file = np.load('embeddings_with_lemmas.npz', allow_pickle=True)\n",
    "embs_vectors = embs_file['vectors']\n",
    "embs_vectors_normed = embs_vectors / np.linalg.norm(embs_vectors, axis=1, keepdims=True)\n",
    "embs_voc = embs_file['voc'].item()\n",
    "\n",
    "embs_voc_by_id = [None for i in range(len(embs_vectors))]\n",
    "for word, idx in embs_voc.items():\n",
    "    if embs_voc_by_id[idx] is None:\n",
    "        embs_voc_by_id[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_indicies(a):\n",
    "    res = []\n",
    "    if isinstance(a, str):\n",
    "        a = a.split()\n",
    "    for w in a:\n",
    "        if w in embs_voc:\n",
    "            res.append(embs_voc[w])\n",
    "        else:\n",
    "            lemma = stemmer.lemmatize(w)[0]\n",
    "            res.append(embs_voc.get(lemma, None))\n",
    "    return res\n",
    "\n",
    "def calc_embs(words):\n",
    "    words = ' '.join(map(normalize, words))\n",
    "    inds = get_w2v_indicies(words)\n",
    "    return [None if i is None else embs_vectors[i] for i in inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление эмбеддинговых расстояний, как в score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_none(array):\n",
    "    res = 0\n",
    "    for el in array:\n",
    "        if el is None:\n",
    "            res += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def greedy_match_embs(a, b, dots=None):\n",
    "    if len(a) == 0:\n",
    "        return len(b)\n",
    "    if len(b) == 0:\n",
    "        return len([x for x in a if x is not None])\n",
    "    # compute dot-product on initial run\n",
    "    if dots is None:\n",
    "        a_none_count = count_none(a)\n",
    "        b_none_count = count_none(b)\n",
    "        \n",
    "        if a_none_count + b_none_count > 0:\n",
    "            # None values don't match anything except other None values\n",
    "            return max(b_none_count - a_none_count, 0) + greedy_match_embs(\n",
    "                [x for x in a if x is not None],\n",
    "                [x for x in b if x is not None]\n",
    "            )\n",
    "        # scale embeddings so that their dot product turns into cosine\n",
    "        a = np.array(a) / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "        b = np.array(b) / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "        dots = np.dot(a, b.T)\n",
    "    # select the closest embeddings\n",
    "    # note: assume None embeddings are filtered out at this point\n",
    "    a_closest, b_closest = np.unravel_index(np.argmax(dots), dots.shape)\n",
    "    min_dist = (1 - dots[a_closest, b_closest]) / 2\n",
    "    \n",
    "    # exclude the matched embeddings from the subsequent iterations\n",
    "    remaining_a_inds = np.arange(len(a)) != a_closest\n",
    "    remaining_b_inds = np.arange(len(b)) != b_closest\n",
    "    \n",
    "    return min_dist + greedy_match_embs(\n",
    "        a[remaining_a_inds], \n",
    "        b[remaining_b_inds], \n",
    "        dots[remaining_a_inds][:, remaining_b_inds]\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_semantic_distance(a, b):\n",
    "    a_embs = calc_embs(a)\n",
    "    b_embs = calc_embs(b)\n",
    "    return np.maximum(greedy_match_embs(a_embs, b_embs), 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция находит самое близкое нетоксичное слово по предпосчитанным эмбеддингам слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache()\n",
    "def find_closest_nontoxic(word, threshold=0.5, measure='cosine'):\n",
    "    threshold = min(toxicity.get(word, threshold), threshold)\n",
    "    word = normalize(word)\n",
    "    word_emb = calc_embs([word])\n",
    "    if word_emb is None or word_emb[0] is None:\n",
    "        return None\n",
    "    word_emb = word_emb[0]\n",
    "    if measure == 'cosine':\n",
    "        word_emb /= np.linalg.norm(word_emb)\n",
    "        dot = embs_vectors_normed.dot(word_emb)\n",
    "    elif measure == 'l2':\n",
    "        dot = -((embs_vectors - word_emb[None]) ** 2).sum(axis=1)\n",
    "    \n",
    "    for i in np.argsort(dot)[::-1]:\n",
    "        other_word = embs_voc_by_id[i]\n",
    "        if other_word != word and toxicity.get(other_word, 1.0) <= threshold:\n",
    "            return other_word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итеративно пытаемся заменить самое токсичное слово на его нетоксичный аналог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detox_iterative(line):\n",
    "    result = normalize(line).split()\n",
    "    while len(result) > 0 and predict_toxicity([' '.join(result)], verbose=False)[0]:\n",
    "        most_toxic = np.argmax(list(map(lambda x: toxicity.get(x, 0.0), result)))\n",
    "        \n",
    "        new_variant = find_closest_nontoxic(result[most_toxic]) or ''\n",
    "        augmented_result = result[:most_toxic] + [new_variant] + result[most_toxic + 1:]\n",
    "        \n",
    "        comparison = predict_toxicity(\n",
    "            [' '.join(result), ' '.join(augmented_result)],\n",
    "            verbose=False,\n",
    "            return_scores=True,\n",
    "            batch_size=10\n",
    "        )\n",
    "        # 0.1 тут \n",
    "        if not new_variant or comparison[0] <= comparison[1] + 0.1:\n",
    "            result = result[:most_toxic] + result[most_toxic + 1:]\n",
    "        else:\n",
    "            result = augmented_result\n",
    "    \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b6b0c00c5645198a4f67ab5c6b73b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fixed_texts = list(map(detox_iterative, tqdm(texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "запишем результат в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_fixed.txt', 'wt') as f:\n",
    "    for text in fixed_texts:\n",
    "        print(text, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скор, если никак не изменять комментарии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dimdi-y/.local/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (2.3.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Loading tokenizer\n",
      "Loading model\n",
      "Loading texts\n",
      "Loading embeddings\n",
      "Scoring\n",
      " 18%|███████▎                                | 92/500.0 [00:03<00:15, 26.27it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 500/500.0 [00:21<00:00, 23.74it/s]\n",
      "2500it [00:12, 194.21it/s]\n",
      "28.07999973873593\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python3.7 score.py public_testset.short.txt public_testset.short.txt  --embeddings embeddings_with_lemmas.npz --model ./trained_roberta/ --device cuda --score -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скор бейзлайна:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dimdi-y/.local/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (2.3.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Loading tokenizer\n",
      "Loading model\n",
      "Loading texts\n",
      "Loading embeddings\n",
      "Scoring\n",
      " 37%|██████████████▍                        | 185/500.0 [00:05<00:09, 34.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 500/500.0 [00:13<00:00, 36.94it/s]\n",
      "2500it [00:34, 73.31it/s] \n",
      "57.84865188082298\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python3.7 score.py public_testset.short.txt baseline_fixed.txt  --embeddings embeddings_with_lemmas.npz --model ./trained_roberta/ --device cuda --score -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
